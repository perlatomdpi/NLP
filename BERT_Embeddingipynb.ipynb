{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-Embeddingipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPp08eN9o9VXMLQRwrEq7Yo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/perlatomdpi/NLP/blob/main/BERT_Embeddingipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXNpFFDtUsIN"
      },
      "source": [
        "# **Using BERT as embedding**\n",
        "Here we use BERT as an embedding layer and w use it as a vector representation of our words. <br>\n",
        "\n",
        "This is a techinque that can be use in any personal model: <br>\n",
        "1 - plug BERT as an embedding layer <br>\n",
        "2 - put the embedding at the beginning of our model <br>\n",
        "\n",
        "These steps improve the model --> more powerful and efficient model. <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZsJ18a2WWss"
      },
      "source": [
        "# **Tokenization**\n",
        "We need to create a BERT layer to have access to meta data for the tokenizer (like vocab size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_dy2_yyUkx9"
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPrnbDGXXcG-"
      },
      "source": [
        "# we need to add CLS and SEP to be able to use BERT embedding\n",
        "# that is why we create a funtion for sentence enconding\n",
        "\n",
        "def encode_sentence(sent):\n",
        "    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTMVpRhnYGZ4"
      },
      "source": [
        "# now we configure all the sentences with the SEP CLS and SEP\n",
        "\n",
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYadXiqBYZit"
      },
      "source": [
        "# **Data creation**\n",
        "BERT takes 3 differents inputs: <br>\n",
        "1 - the sentences that we encoded here above <br>\n",
        "2 - the list of masks and paddng where necessary <br>\n",
        "3 - the segment input --> 0= first sentence, 1= second sentence --> this process is important to recognize padding tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCq6BqhAYWXq"
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)                        # 1 - return the id of each token\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)                 # 2 - padding mask: = if it is a padding, 1= if not padding token\n",
        "\n",
        "\n",
        "\n",
        "def get_segments(tokens):                         # 3 - segment input\n",
        "    seg_ids = []                                  # create an empty list of our token that will be returned\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":                        # if we passed the first sentence --> switch the token:\n",
        "            current_seg_id = 1-current_seg_id     # turns 1 into 0 and vice versa --> this recognize padding tokens\n",
        "    return seg_ids"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}