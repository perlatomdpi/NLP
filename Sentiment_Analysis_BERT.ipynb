{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment-Analysis-BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMWesDnKNs5RjEa2lv+jLgN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/perlatomdpi/NLP/blob/main/Sentiment_Analysis_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0WxTtlr1NOX"
      },
      "source": [
        "# **Sentiment analysis model with BERT**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlIryEXq1vU-"
      },
      "source": [
        "# **Import dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evtZvzDp1MGO"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re # advanced text pre-processing\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup # encode text\n",
        "import random\n",
        "\n",
        "from google.colab import drive # get data from drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef7I-zp311H8"
      },
      "source": [
        "# use tf2\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub # to download the weights of bert\n",
        "from tensorflow.keras import layers # to create cnn layers\n",
        "import bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct1bSo-Z1yV_"
      },
      "source": [
        "# **Data pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRtwDzru16mQ"
      },
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "cols = [\"sentiment\", \"id\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read.csv(\n",
        "    \"content/drive/My Drive/.../BERT/data/train.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\", \n",
        "    encoding=\"latin1\"\n",
        ")\n",
        "\n",
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True) # inplace garantee that new lighter data are loaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MbKYA5m2BOS"
      },
      "source": [
        "# **Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZw-buKU2GUB"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", '',tweet)   # replace a substring with another\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", '', tweet)\n",
        "    tweet = re.sub(r\"[^A-Za-z.!?]\", '', tweet)\n",
        "    tweet = re.sub(r\" +\", '', tweet)\n",
        "    return tweet\n",
        "\n",
        "data_clean = [clean_tweet(tweet) for tweet in data.text]\n",
        "\n",
        "data_labels = data.sentiment.values # get the value of the sentiment\n",
        "data_labels[data_labels == 4] = 1   # in the data 4 is for positve and we converted it to 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14YfeheZ2N2n"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0AHp02U2OpE"
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer # call bert module\n",
        "\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False) # create bert layer - hub is were the pre-trainined model is stored\n",
        "                                             # here we use the light version of bert\n",
        "                                             # L-12: 12 encoders\n",
        "                                             # trainable=false --> we don't use bert for fine-tuning weights\n",
        "\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() # give access to the vocab file as numpy\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() # lower case\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case) # create the tokenizer\n",
        "\n",
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) # id: each token is related to a number\n",
        "\n",
        "# per each sentence run tokenizer and give valid input too our model\n",
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8VseO_t2VUu"
      },
      "source": [
        "# **Create dataset and use it as input for the trainin model**\n",
        "\n",
        "The BERT model receives a fixed length of sentence as input. Usually the maximum length of a sentence depends on the data we are working on. For sentences that are shorter than this maximum length, we will have to add paddings (empty tokens) to the sentences to make up the length.\n",
        "\n",
        "We will create padded batches (so we pad sentences for each batch independently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW7UrwwN2XxE"
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)] # iterate over the data\n",
        "\n",
        "random.shuffle(data_with_len) # shuffle it: first half is negative sentiment, second half is positive sentiment\n",
        "                              # so with shuffle we have negative and positive sentences mixed and not only positve or negative\n",
        "\n",
        "data_with_len.sort(key=lambda x: x[2]) # sort according to the length\n",
        "                                       # per each element x --> [sent, data_labels[i], len(sent)] access to the len(sent)\n",
        "                                       # so len(sent) is the criteria for sorting\n",
        "\n",
        "sorted_all = [(sent_lab[0], sent_lab[1])\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7] # get sentence with 7 words as minimum\n",
        "\n",
        "\n",
        "# create dataset via generator --> generator give element one after the other \n",
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32)) # interger because the token are the token id\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ())) # None: add empty token to make up the length\n",
        "\n",
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE) # give the number of batches\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10                   # get 10 test\n",
        "all_batched.shuffle(NB_BATCHES)                      # shuffle in order to mix positive and negative lables\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)     # create test data\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)    # create train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2ZSes6t2g-2"
      },
      "source": [
        "# **Model building**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_vnNw8a2pA3"
      },
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim=128,\n",
        "                 nb_filters=50,\n",
        "                 FFN_units=512,\n",
        "                 nb_classes=2,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                 name=\"dcnn\"):\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size,\n",
        "                                          emb_dim)\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
        "                                    kernel_size=2,\n",
        "                                    padding=\"valid\",\n",
        "                                    activation=\"relu\")\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        x = self.embedding(inputs)\n",
        "        x_1 = self.bigram(x) # batch_size, nb_filters, seq_len-1)\n",
        "        x_1 = self.pool(x_1) # (batch_size, nb_filters)\n",
        "        x_2 = self.trigram(x) # batch_size, nb_filters, seq_len-2)\n",
        "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
        "        x_3 = self.fourgram(x) # batch_size, nb_filters, seq_len-3)\n",
        "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
        "        \n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)\n",
        "        merged = self.dropout(merged, training)\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}