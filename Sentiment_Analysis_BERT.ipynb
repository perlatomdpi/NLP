{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment-Analysis-BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNWHrS0qGeVyUdEidj7JaD1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/perlatomdpi/NLP/blob/main/Sentiment_Analysis_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0WxTtlr1NOX"
      },
      "source": [
        "# **Sentiment analysis model with BERT**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlIryEXq1vU-"
      },
      "source": [
        "# **Import dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evtZvzDp1MGO"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re # advanced text pre-processing\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup # encode text\n",
        "import random\n",
        "\n",
        "from google.colab import drive # get data from drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef7I-zp311H8"
      },
      "source": [
        "# use tf2\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub # to download the weights of bert\n",
        "from tensorflow.keras import layers # to create cnn layers\n",
        "import bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct1bSo-Z1yV_"
      },
      "source": [
        "# **Data pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRtwDzru16mQ"
      },
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "cols = [\"sentiment\", \"id\", \"query\", \"user\", \"text\"]\n",
        "data = pd.read.csv(\n",
        "    \"content/drive/My Drive/.../BERT/data/train.csv\",\n",
        "    header=None,\n",
        "    names=cols,\n",
        "    engine=\"python\", \n",
        "    encoding=\"latin1\"\n",
        ")\n",
        "\n",
        "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
        "          axis=1,\n",
        "          inplace=True) # inplace garantee that new lighter data are loaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MbKYA5m2BOS"
      },
      "source": [
        "# **Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZw-buKU2GUB"
      },
      "source": [
        "def clean_tweet(tweet):\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]+\", '',tweet)   # replace a substring with another\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", '', tweet)\n",
        "    tweet = re.sub(r\"[^A-Za-z.!?]\", '', tweet)\n",
        "    tweet = re.sub(r\" +\", '', tweet)\n",
        "    return tweet\n",
        "\n",
        "data_clean = [clean_tweet(tweet) for tweet in data.text]\n",
        "\n",
        "data_labels = data.sentiment.values # get the value of the sentiment\n",
        "data_labels[data_labels == 4] = 1   # in the data 4 is for positve and we converted it to 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14YfeheZ2N2n"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0AHp02U2OpE"
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer # call bert module\n",
        "\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False) # create bert layer - hub is were the pre-trainined model is stored\n",
        "                                             # here we use the light version of bert\n",
        "                                             # L-12: 12 encoders\n",
        "                                             # trainable=false --> we don't use bert for fine-tuning weights\n",
        "\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() # give access to the vocab file as numpy\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() # lower case\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case) # create the tokenizer\n",
        "\n",
        "def encode_sentence(sent):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) # id: each token is related to a number\n",
        "\n",
        "# per each sentence run tokenizer and give valid input too our model\n",
        "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8VseO_t2VUu"
      },
      "source": [
        "# **Create dataset and use it as input for the trainin model**\n",
        "\n",
        "The BERT model receives a fixed length of sentence as input. Usually the maximum length of a sentence depends on the data we are working on. For sentences that are shorter than this maximum length, we will have to add paddings (empty tokens) to the sentences to make up the length.\n",
        "\n",
        "We will create padded batches (so we pad sentences for each batch independently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW7UrwwN2XxE"
      },
      "source": [
        "data_with_len = [[sent, data_labels[i], len(sent)]\n",
        "                 for i, sent in enumerate(data_inputs)] # iterate over the data\n",
        "\n",
        "random.shuffle(data_with_len) # shuffle it: first half is negative sentiment, second half is positive sentiment\n",
        "                              # so with shuffle we have negative and positive sentences mixed and not only positve or negative\n",
        "\n",
        "data_with_len.sort(key=lambda x: x[2]) # sort according to the length\n",
        "                                       # per each element x --> [sent, data_labels[i], len(sent)] access to the len(sent)\n",
        "                                       # so len(sent) is the criteria for sorting\n",
        "\n",
        "sorted_all = [(sent_lab[0], sent_lab[1])\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 7] # get sentence with 7 words as minimum\n",
        "\n",
        "\n",
        "# create dataset via generator --> generator give element one after the other \n",
        "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
        "                                             output_types=(tf.int32, tf.int32)) # interger because the token are the token id\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ())) # None: add empty token to make up the length\n",
        "\n",
        "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE) # give the number of batches\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10                   # get 10 test\n",
        "all_batched.shuffle(NB_BATCHES)                      # shuffle in order to mix positive and negative lables\n",
        "test_dataset = all_batched.take(NB_BATCHES_TEST)     # create test data\n",
        "train_dataset = all_batched.skip(NB_BATCHES_TEST)    # create train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2ZSes6t2g-2"
      },
      "source": [
        "# **Model building**\n",
        "Here we use embedding representation of vectors that we can use for our model. <br>\n",
        "The most important part of CNN is the Convolutional part. Here we moltiply the inpt list vector by the Feature Detector to obtain the so-called Feature Map. The key part is the Feature Detector and this is the part that we have to train. <br>\n",
        "So, our CNN learns to detect or useful or irrelavant features by tuning the coefficient of the Feature Detection matrix also called Convulational Filters. Tipically more than one convolutional fiters are used.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_vnNw8a2pA3"
      },
      "source": [
        "class DCNN(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 emb_dim=128,\n",
        "                 nb_filters=50,                             # convolutional fiter --> feature detector --> 50 of size 2, 50 of size 3, 50 of size 4\n",
        "                 FFN_units=512,                             # number of hidden units of our dense layer\n",
        "                 nb_classes=2,                              # level of the independent variable: positive, negative sentiment\n",
        "                 dropout_rate=0.1,                          # shut-down neurons to prevent overfitting                         \n",
        "                 training=False,\n",
        "                 name=\"dcnn\"):\n",
        "        super(DCNN, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size,       # --> feature detector --> 50 of size 2\n",
        "                                          emb_dim)          # convert embedding to vector\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,     # focus on two consecutive words --> Conv1D: shift vertically the feature detector\n",
        "                                    kernel_size=2,          # 2 because is bi.gram\n",
        "                                    padding=\"valid\",        # when we have steps more than 1 during the feature detection iteration --> we could get our of the max range\n",
        "                                    activation=\"relu\")      # Rectified Linear Unit get-rid of negative values that will be set to zero \n",
        "\n",
        "        self.trigram = layers.Conv1D(filters=nb_filters,    # --> feature detector --> 50 of size 3\n",
        "                                     kernel_size=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     activation=\"relu\")\n",
        "        \n",
        "        self.fourgram = layers.Conv1D(filters=nb_filters,   # --> feature detector --> 50 of size 4\n",
        "                                      kernel_size=4,\n",
        "                                      padding=\"valid\",\n",
        "                                      activation=\"relu\")\n",
        "        \n",
        "        self.pool = layers.GlobalMaxPool1D()                # create the layers that take the feature max per each feature detector 2,3,4\n",
        "        \n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")  # feedforward neural network part --> we need 2 dense layers \n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)                 # we use drop-out to shut down neurons --> prevent overfitting\n",
        "\n",
        "\n",
        "\n",
        "        # the number of dense layer is equal to the number of the level of the dependent variable \n",
        "        # here we have a sentiment analysis with 2 levels --> positive/negative\n",
        "        if nb_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\") # sigmoid is used when we have only 2 levels od dependent variable \n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\n",
        "                                           activation=\"softmax\") # if we have more than 2 levels dependent variable the softmax function is used instead\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    # the call funtion is used to make predition or to prain the model\n",
        "    # with the parameter \"training\" we specify that we are in the training phase\n",
        "    # note that the drop-out is only used during training and not when we make prediction\n",
        "    def call(self, inputs, training):   \n",
        "        x = self.embedding(inputs)    # call the embedding\n",
        "\n",
        "        # create the set of convolutional layers:\n",
        "        #\n",
        "        # for each of our 50 feature detectors of size 2 (bi-gram) --> we get the maxium activation (pooling)\n",
        "        x_1 = self.bigram(x)    # batch_size, nb_filters, seq_len-1)\n",
        "        x_1 = self.pool(x_1)    # (batch_size, nb_filters)\n",
        "\n",
        "        # for each of our 50 feature detectors of size 3 (tri-gram) --> we get the maxium activation (pooling)\n",
        "        x_2 = self.trigram(x)   # batch_size, nb_filters, seq_len-2)\n",
        "        x_2 = self.pool(x_2)    # (batch_size, nb_filters)\n",
        "\n",
        "        # for each of our 50 feature detectors of size 4 (four-gram) --> we get the maxium activation (pooling)\n",
        "        x_3 = self.fourgram(x)  # batch_size, nb_filters, seq_len-3)\n",
        "        x_3 = self.pool(x_3)    # (batch_size, nb_filters)\n",
        "        \n",
        "\n",
        "        # concatenate all the results and apply them to the dense layer\n",
        "        merged = tf.concat([x_1, x_2, x_3], axis=-1)    # (batch_size, 3 * nb_filters)\n",
        "        merged = self.dense_1(merged)                   # apply dense layer\n",
        "        merged = self.dropout(merged, training)         # apply drop-out\n",
        "        output = self.last_dense(merged)\n",
        "        \n",
        "        return output\n",
        "\n",
        "        # Here we have built and define our CNN model\n",
        "        # Now we are ready to train it and create an instance of this classs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUQqVY4U4fRz"
      },
      "source": [
        "# **Training**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfUvgTzv4i_c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}